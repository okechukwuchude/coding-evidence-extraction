{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertConfig, BertForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_df = pd.read_csv('/home/chudeo/coding-evidence-extraction-main/33k_sentence.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id    625510\n",
       "words          625262\n",
       "labels         625510\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence_id      0\n",
       "words          248\n",
       "labels           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for null values\n",
    "token_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3994101/3831777956.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  data = token_df.fillna(method='ffill')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>artifact</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Probable</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>sinus</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id     words labels\n",
       "0            0  Baseline      O\n",
       "1            0  artifact      O\n",
       "2            0         .      O\n",
       "3            1  Probable      O\n",
       "4            1     sinus      B"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = token_df.fillna(method='ffill')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>words</th>\n",
       "      <th>labels</th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Baseline</td>\n",
       "      <td>O</td>\n",
       "      <td>Baseline artifact .</td>\n",
       "      <td>O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>artifact</td>\n",
       "      <td>O</td>\n",
       "      <td>Baseline artifact .</td>\n",
       "      <td>O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>Baseline artifact .</td>\n",
       "      <td>O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Probable</td>\n",
       "      <td>O</td>\n",
       "      <td>Probable sinus tachycardia with atrial prematu...</td>\n",
       "      <td>O,B,I,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>sinus</td>\n",
       "      <td>B</td>\n",
       "      <td>Probable sinus tachycardia with atrial prematu...</td>\n",
       "      <td>O,B,I,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id     words labels  \\\n",
       "0            0  Baseline      O   \n",
       "1            0  artifact      O   \n",
       "2            0         .      O   \n",
       "3            1  Probable      O   \n",
       "4            1     sinus      B   \n",
       "\n",
       "                                            sentence      word_labels  \n",
       "0                                Baseline artifact .            O,O,O  \n",
       "1                                Baseline artifact .            O,O,O  \n",
       "2                                Baseline artifact .            O,O,O  \n",
       "3  Probable sinus tachycardia with atrial prematu...  O,B,I,O,O,O,O,O  \n",
       "4  Probable sinus tachycardia with atrial prematu...  O,B,I,O,O,O,O,O  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create a new column called \"sentence\" which groups the words by sentence\n",
    "data['sentence'] = data[['sentence_id','words','labels']].groupby(['sentence_id'])['words'].transform(lambda x: ' '.join(x))\n",
    "# let's also create a new column called \"word_labels\" which groups the tags by sentence\n",
    "data['word_labels'] = data[['sentence_id','words','labels']].groupby(['sentence_id'])['labels'].transform(lambda x: ','.join(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B': 1, 'I': 2}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "label2id = {k: v for v, k in enumerate(data.labels.unique())}\n",
    "id2label = {v: k for v, k in enumerate(data.labels.unique())}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>word_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline artifact .</td>\n",
       "      <td>O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Probable sinus tachycardia with atrial prematu...</td>\n",
       "      <td>O,B,I,O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Low limb lead voltage .</td>\n",
       "      <td>O,O,O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Leftward axis .</td>\n",
       "      <td>O,O,O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Late R wave progression .</td>\n",
       "      <td>O,O,O,O,O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence      word_labels\n",
       "0                                Baseline artifact .            O,O,O\n",
       "1  Probable sinus tachycardia with atrial prematu...  O,B,I,O,O,O,O,O\n",
       "2                            Low limb lead voltage .        O,O,O,O,O\n",
       "3                                    Leftward axis .            O,O,O\n",
       "4                          Late R wave progression .        O,O,O,O,O"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11262"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Probable sinus tachycardia with atrial premature beats .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O,B,I,O,O,O,O,O'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[1].word_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Sampled sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "# Separate majority and minority classes\n",
    "majority_class = data[data['word_labels'].apply(lambda x: all(label == 'O' for label in x.split(',')))]\n",
    "minority_class = data[~data['word_labels'].apply(lambda x: all(label == 'O' for label in x.split(',')))]\n",
    "\n",
    "# Downsample majority class\n",
    "majority_downsampled = resample(majority_class, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=len(minority_class),  # match minority class size\n",
    "                                 random_state=42)  # reproducible results\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "balanced_data = pd.concat([majority_downsampled, minority_class])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4378"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(balanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = balanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract words and labels from DataFrame\n",
    "split_data = {\n",
    "    \"words\": [word for sent in data[\"sentence\"].str.split() for word in sent],\n",
    "    \"labels\": [label.split(',') for label in data[\"word_labels\"]]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into sentences and labels\n",
    "sentences = split_data[\"words\"]\n",
    "labels = split_data[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOG_INF = 10e5\n",
    "\n",
    "\n",
    "class BertCrf(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int,\n",
    "        bert_name: str,\n",
    "        dropout: float = 0.2,\n",
    "        use_crf: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_crf = use_crf\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(bert_name)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2label = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_labels))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_labels))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_labels, num_labels))\n",
    "\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "\n",
    "    def _compute_log_denominator(self, features: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = features.shape[0]\n",
    "\n",
    "        log_score_over_all_seq = self.start_transitions + features[0]\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            next_log_score_over_all_seq = torch.logsumexp(\n",
    "                log_score_over_all_seq.unsqueeze(2) + self.transitions + features[i].unsqueeze(1),\n",
    "                dim=1,\n",
    "            )\n",
    "            log_score_over_all_seq = torch.where(\n",
    "                mask[i].unsqueeze(1),\n",
    "                next_log_score_over_all_seq,\n",
    "                log_score_over_all_seq,\n",
    "            )\n",
    "        log_score_over_all_seq += self.end_transitions\n",
    "        return torch.logsumexp(log_score_over_all_seq, dim=1)\n",
    "\n",
    "    def _compute_log_numerator(self, features: torch.Tensor, labels: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len, bs, _ = features.shape\n",
    "\n",
    "        score_over_seq = self.start_transitions[labels[0]] + features[0, torch.arange(bs), labels[0]]\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            score_over_seq += (\n",
    "                self.transitions[labels[i - 1], labels[i]] + features[i, torch.arange(bs), labels[i]]\n",
    "            ) * mask[i]\n",
    "        seq_lens = mask.sum(dim=0) - 1\n",
    "        last_tags = labels[seq_lens.long(), torch.arange(bs)]\n",
    "        score_over_seq += self.end_transitions[last_tags]\n",
    "        return score_over_seq\n",
    "\n",
    "    def get_bert_features(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden = self.bert(input_ids, attention_mask=attention_mask)[\"last_hidden_state\"]\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.hidden2label(hidden), hidden\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        features, _ = self.get_bert_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        if self.use_crf:\n",
    "            features = torch.swapaxes(features, 0, 1)\n",
    "            attention_mask = torch.swapaxes(attention_mask, 0, 1)\n",
    "            labels = torch.swapaxes(labels, 0, 1)\n",
    "\n",
    "            log_numerator = self._compute_log_numerator(features=features, labels=labels, mask=attention_mask)\n",
    "            log_denominator = self._compute_log_denominator(features=features, mask=attention_mask)\n",
    "\n",
    "            return torch.mean(log_denominator - log_numerator)\n",
    "        else:\n",
    "            return self.cross_entropy(\n",
    "                features.flatten(end_dim=1),\n",
    "                torch.where(attention_mask.bool(), labels, -100).flatten(end_dim=1),\n",
    "            )\n",
    "\n",
    "    def _viterbi_decode(self, features: torch.Tensor, mask: torch.Tensor) -> List[List[int]]:\n",
    "        seq_len, bs, _ = features.shape\n",
    "\n",
    "        log_score_over_all_seq = self.start_transitions + features[0]\n",
    "\n",
    "        backpointers = torch.empty_like(features)\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            next_log_score_over_all_seq = (\n",
    "                log_score_over_all_seq.unsqueeze(2) + self.transitions + features[i].unsqueeze(1)\n",
    "            )\n",
    "\n",
    "            next_log_score_over_all_seq, indices = next_log_score_over_all_seq.max(dim=1)\n",
    "\n",
    "            log_score_over_all_seq = torch.where(\n",
    "                mask[i].unsqueeze(1),\n",
    "                next_log_score_over_all_seq,\n",
    "                log_score_over_all_seq,\n",
    "            )\n",
    "            backpointers[i] = indices\n",
    "\n",
    "        backpointers = backpointers[1:].int()\n",
    "\n",
    "        log_score_over_all_seq += self.end_transitions\n",
    "        seq_lens = mask.sum(dim=0) - 1\n",
    "\n",
    "        best_paths = []\n",
    "        for seq_ind in range(bs):\n",
    "            best_label_id = torch.argmax(log_score_over_all_seq[seq_ind]).item()\n",
    "            best_path = [best_label_id]\n",
    "\n",
    "            for backpointer in reversed(backpointers[: seq_lens[seq_ind]]):\n",
    "                best_path.append(backpointer[seq_ind][best_path[-1]].item())\n",
    "\n",
    "            best_path.reverse()\n",
    "            best_paths.append(best_path)\n",
    "\n",
    "        return best_paths\n",
    "\n",
    "    def decode(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> List[List[int]]:\n",
    "        features, _ = self.get_bert_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        if self.use_crf:\n",
    "            features = torch.swapaxes(features, 0, 1)\n",
    "            mask = torch.swapaxes(attention_mask, 0, 1)\n",
    "            return self._viterbi_decode(features=features, mask=mask)\n",
    "        else:\n",
    "            labels = torch.argmax(features, dim=2)\n",
    "            predictions = []\n",
    "            for i in range(len(labels)):\n",
    "                predictions.append(labels[i][attention_mask[i]].tolist())\n",
    "            return predictions\n",
    "\n",
    "    def save_to(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_from(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_device(\n",
    "    dict: Dict[str, torch.Tensor],\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    for key, value in dict.items():\n",
    "        dict[key] = value.to(device)\n",
    "    return dict\n",
    "\n",
    "\n",
    "def draw_plots(loss_history: List[float], f1: List[float]):\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    f, (ax1, ax2) = plt.subplots(2)\n",
    "    f.set_figwidth(15)\n",
    "    f.set_figheight(10)\n",
    "\n",
    "    ax1.set_title(\"training loss\")\n",
    "    ax2.set_title(\"f1 micro\")\n",
    "\n",
    "    ax1.plot(loss_history)\n",
    "    ax2.plot(f1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if len(loss_history) > 0:\n",
    "        print(f\"Current loss: {loss_history[-1]}\")\n",
    "    if len(f1) > 0:\n",
    "        print(f\"Current f1: {f1[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NerDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts_path: str):\n",
    "        self.tokenized_texts = []\n",
    "        with open(tokenized_texts_path, \"r\") as tokenized_texts_file:\n",
    "            for line in tokenized_texts_file:\n",
    "                self.tokenized_texts.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, List]:\n",
    "        return self.tokenized_texts[index]\n",
    "\n",
    "    def collate_function(self, batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [torch.tensor(item[\"input_ids\"]) for item in batch]\n",
    "        labels = [torch.tensor(item[\"labels\"]) for item in batch]\n",
    "        attention_mask = [torch.ones(len(item)) for item in input_ids]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": pad_sequence(input_ids, batch_first=True),\n",
    "            \"labels\": pad_sequence(labels, batch_first=True),\n",
    "            \"attention_mask\": pad_sequence(attention_mask, batch_first=True),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ner(\n",
    "    num_labels: int,\n",
    "    bert_name: str,\n",
    "    train_tokenized_texts_path: str,\n",
    "    test_tokenized_texts_path: str,\n",
    "    dropout: float,\n",
    "    batch_size: int,\n",
    "    epochs: int,\n",
    "    log_every: int,\n",
    "    lr_bert: float,\n",
    "    lr_new_layers: float,\n",
    "    use_crf: bool = True,\n",
    "    save_to: Optional[str] = None,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    model = BertCrf(num_labels, bert_name, dropout=dropout, use_crf=use_crf)\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    train_dataset = NerDataset(train_tokenized_texts_path)\n",
    "    train_data_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=train_dataset.collate_function,\n",
    "    )\n",
    "\n",
    "    test_dataset = NerDataset(test_tokenized_texts_path)\n",
    "    test_data_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=test_dataset.collate_function,\n",
    "    )\n",
    "\n",
    "    optimizer = Adam(\n",
    "        [\n",
    "            {\"params\": model.start_transitions},\n",
    "            {\"params\": model.end_transitions},\n",
    "            {\"params\": model.hidden2label.parameters()},\n",
    "            {\"params\": model.transitions},\n",
    "            {\"params\": model.bert.parameters(), \"lr\": lr_bert},\n",
    "        ],\n",
    "        lr=lr_new_layers,\n",
    "    )\n",
    "\n",
    "    loss_history = []\n",
    "    f1 = []\n",
    "\n",
    "    step = 0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for batch in tqdm(train_data_loader):\n",
    "            step += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            batch = dict_to_device(batch, device)\n",
    "\n",
    "            loss = model(**batch)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "            if step % log_every == 0:\n",
    "                model.eval()\n",
    "                predictions = []\n",
    "                ground_truth = []\n",
    "                with torch.no_grad():\n",
    "                    for batch in test_data_loader:\n",
    "                        labels = batch[\"labels\"]\n",
    "                        del batch[\"labels\"]\n",
    "                        batch = dict_to_device(batch)\n",
    "\n",
    "                        prediction = model.decode(**batch)\n",
    "\n",
    "                        flatten_prediction = [item for sublist in prediction for item in sublist]\n",
    "                        flatten_labels = torch.masked_select(labels, batch[\"attention_mask\"].bool()).tolist()\n",
    "\n",
    "                        predictions.extend(flatten_prediction)\n",
    "                        ground_truth.extend(flatten_labels)\n",
    "                f1_micro = f1_score(ground_truth, predictions, average=\"micro\")\n",
    "                f1.append(f1_micro)\n",
    "                model.train()\n",
    "\n",
    "            draw_plots(loss_history, f1)\n",
    "            print(f\"Epoch {epoch}/{epochs}\")\n",
    "    if save_to is not None:\n",
    "        model.save_to(save_to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
