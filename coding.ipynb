{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label2id: {'I': 0, 'B': 1, 'O': 2}\n",
      "id2label: {0: 'I', 1: 'B', 2: 'O'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(\"/home/chudeo/coding-evidence-extraction-main/pos_sentence.csv\")\n",
    "\n",
    "# Extract unique labels from the word_labels column\n",
    "unique_labels = set()\n",
    "for labels in df['word_labels']:\n",
    "    unique_labels.update(labels.split(','))\n",
    "\n",
    "# Generate label2id and id2label dictionaries\n",
    "label2id = {label: id for id, label in enumerate(unique_labels)}\n",
    "id2label = {id: label for label, id in label2id.items()}\n",
    "\n",
    "# Example usage:\n",
    "print(\"label2id:\", label2id)\n",
    "print(\"id2label:\", id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags_with_positions(labels: List[str], id2label: Dict[int, str]) -> List[Dict[str, Union[str, List[int]]]]:\n",
    "    tags_pos = []\n",
    "    ind = 0\n",
    "    while ind < len(labels):\n",
    "        if id2label[labels[ind]].startswith(\"B\"):\n",
    "            tag = id2label[labels[ind]].split(\"-\")[1]\n",
    "            start_pos = ind\n",
    "            ind += 1\n",
    "            while ind < len(labels) and id2label[labels[ind]].startswith(\"I\"):\n",
    "                ind += 1\n",
    "            end_pos = ind\n",
    "            tags_pos.append({\"tag\": tag, \"pos\": [start_pos, end_pos]})\n",
    "        else:\n",
    "            ind += 1\n",
    "    return tags_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector_from_segment(embeddings: torch.Tensor, start_pos: int, end_pos: int) -> torch.Tensor:\n",
    "    return embeddings[start_pos:end_pos].mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv(file_path: str):\n",
    "    df = pd.read_csv(file_path)\n",
    "    for _, row in df.iterrows():\n",
    "        sentence = row['sentence']\n",
    "        word_labels = row['word_labels'].split(',')\n",
    "        pos_tags = row['pos_tags'].split(',')\n",
    "        # Assuming you have id2label dictionary defined somewhere\n",
    "        tags_with_positions = get_tags_with_positions(word_labels, id2label)\n",
    "        # Perform further processing as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'O'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m csv_file_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/home/chudeo/coding-evidence-extraction-main/pos_sentence.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m process_csv(csv_file_path)\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mprocess_csv\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m pos_tags \u001b[39m=\u001b[39m row[\u001b[39m'\u001b[39m\u001b[39mpos_tags\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Assuming you have id2label dictionary defined somewhere\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tags_with_positions \u001b[39m=\u001b[39m get_tags_with_positions(word_labels, id2label)\n",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m, in \u001b[0;36mget_tags_with_positions\u001b[0;34m(labels, id2label)\u001b[0m\n\u001b[1;32m      3\u001b[0m ind \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mwhile\u001b[39;00m ind \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(labels):\n\u001b[0;32m----> 5\u001b[0m     \u001b[39mif\u001b[39;00m id2label[labels[ind]]\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m      6\u001b[0m         tag \u001b[39m=\u001b[39m id2label[labels[ind]]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m         start_pos \u001b[39m=\u001b[39m ind\n",
      "\u001b[0;31mKeyError\u001b[0m: 'O'"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"/home/chudeo/coding-evidence-extraction-main/pos_sentence.csv\"\n",
    "process_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_INF = 10e5\n",
    "\n",
    "\n",
    "class BertCrf(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int,\n",
    "        bert_name: str,\n",
    "        dropout: float = 0.2,\n",
    "        use_crf: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.use_crf = use_crf\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(bert_name)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2label = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "        self.start_transitions = nn.Parameter(torch.empty(num_labels))\n",
    "        self.end_transitions = nn.Parameter(torch.empty(num_labels))\n",
    "        self.transitions = nn.Parameter(torch.empty(num_labels, num_labels))\n",
    "\n",
    "        nn.init.uniform_(self.start_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.end_transitions, -0.1, 0.1)\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "\n",
    "    def _compute_log_denominator(self, features: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = features.shape[0]\n",
    "\n",
    "        log_score_over_all_seq = self.start_transitions + features[0]\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            next_log_score_over_all_seq = torch.logsumexp(\n",
    "                log_score_over_all_seq.unsqueeze(2) + self.transitions + features[i].unsqueeze(1),\n",
    "                dim=1,\n",
    "            )\n",
    "            log_score_over_all_seq = torch.where(\n",
    "                mask[i].unsqueeze(1),\n",
    "                next_log_score_over_all_seq,\n",
    "                log_score_over_all_seq,\n",
    "            )\n",
    "        log_score_over_all_seq += self.end_transitions\n",
    "        return torch.logsumexp(log_score_over_all_seq, dim=1)\n",
    "\n",
    "    def _compute_log_numerator(self, features: torch.Tensor, labels: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len, bs, _ = features.shape\n",
    "\n",
    "        score_over_seq = self.start_transitions[labels[0]] + features[0, torch.arange(bs), labels[0]]\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            score_over_seq += (\n",
    "                self.transitions[labels[i - 1], labels[i]] + features[i, torch.arange(bs), labels[i]]\n",
    "            ) * mask[i]\n",
    "        seq_lens = mask.sum(dim=0) - 1\n",
    "        last_tags = labels[seq_lens.long(), torch.arange(bs)]\n",
    "        score_over_seq += self.end_transitions[last_tags]\n",
    "        return score_over_seq\n",
    "\n",
    "    def get_bert_features(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        hidden = self.bert(input_ids, attention_mask=attention_mask)[\"last_hidden_state\"]\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.hidden2label(hidden), hidden\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        features, _ = self.get_bert_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        if self.use_crf:\n",
    "            features = torch.swapaxes(features, 0, 1)\n",
    "            attention_mask = torch.swapaxes(attention_mask, 0, 1)\n",
    "            labels = torch.swapaxes(labels, 0, 1)\n",
    "\n",
    "            log_numerator = self._compute_log_numerator(features=features, labels=labels, mask=attention_mask)\n",
    "            log_denominator = self._compute_log_denominator(features=features, mask=attention_mask)\n",
    "\n",
    "            return torch.mean(log_denominator - log_numerator)\n",
    "        else:\n",
    "            return self.cross_entropy(\n",
    "                features.flatten(end_dim=1),\n",
    "                torch.where(attention_mask.bool(), labels, -100).flatten(end_dim=1),\n",
    "            )\n",
    "\n",
    "    def _viterbi_decode(self, features: torch.Tensor, mask: torch.Tensor) -> List[List[int]]:\n",
    "        seq_len, bs, _ = features.shape\n",
    "\n",
    "        log_score_over_all_seq = self.start_transitions + features[0]\n",
    "\n",
    "        backpointers = torch.empty_like(features)\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            next_log_score_over_all_seq = (\n",
    "                log_score_over_all_seq.unsqueeze(2) + self.transitions + features[i].unsqueeze(1)\n",
    "            )\n",
    "\n",
    "            next_log_score_over_all_seq, indices = next_log_score_over_all_seq.max(dim=1)\n",
    "\n",
    "            log_score_over_all_seq = torch.where(\n",
    "                mask[i].unsqueeze(1),\n",
    "                next_log_score_over_all_seq,\n",
    "                log_score_over_all_seq,\n",
    "            )\n",
    "            backpointers[i] = indices\n",
    "\n",
    "        backpointers = backpointers[1:].int()\n",
    "\n",
    "        log_score_over_all_seq += self.end_transitions\n",
    "        seq_lens = mask.sum(dim=0) - 1\n",
    "\n",
    "        best_paths = []\n",
    "        for seq_ind in range(bs):\n",
    "            best_label_id = torch.argmax(log_score_over_all_seq[seq_ind]).item()\n",
    "            best_path = [best_label_id]\n",
    "\n",
    "            for backpointer in reversed(backpointers[: seq_lens[seq_ind]]):\n",
    "                best_path.append(backpointer[seq_ind][best_path[-1]].item())\n",
    "\n",
    "            best_path.reverse()\n",
    "            best_paths.append(best_path)\n",
    "\n",
    "        return best_paths\n",
    "\n",
    "    def decode(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> List[List[int]]:\n",
    "        features, _ = self.get_bert_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        attention_mask = attention_mask.bool()\n",
    "\n",
    "        if self.use_crf:\n",
    "            features = torch.swapaxes(features, 0, 1)\n",
    "            mask = torch.swapaxes(attention_mask, 0, 1)\n",
    "            return self._viterbi_decode(features=features, mask=mask)\n",
    "        else:\n",
    "            labels = torch.argmax(features, dim=2)\n",
    "            predictions = []\n",
    "            for i in range(len(labels)):\n",
    "                predictions.append(labels[i][attention_mask[i]].tolist())\n",
    "            return predictions\n",
    "\n",
    "    def save_to(self, path: str):\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load_from(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intel",
   "language": "python",
   "name": "intel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
